#!/usr/bin/env python
# coding: utf-8

# In[2]:


import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

df = pd.read_csv("2108387_pokemon.csv") 

height_m = np.array(df['height_m'][0:421])
weight_kg = np.array(df['weight_kg'][0:421])
X = height_m
y = weight_kg

plt.figure()
plt.plot(height_m, weight_kg,'b.')
plt.xlabel('height (m)')
plt.ylabel('weight (kg)')
plt.title("height vs weight")


# In[3]:


from sklearn.model_selection import train_test_split

# In the first step we will split the data in training and remaining dataset
X_train, X_rem, y_train, y_rem = train_test_split(X,y, random_state=42, train_size=0.8)

# Now since we want the valid and test size to be equal (10% each of overall data). 
# we have to define valid_size=0.5 (that is 50% of remaining data)
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, random_state=42, test_size=0.5)

print(X_train.shape, y_train.shape)
print(X_valid.shape, y_valid.shape)
print(X_test.shape, y_test.shape)


# In[4]:


import mglearn
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

X = X.reshape(-1,1)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
lr = LinearRegression().fit(X_train, y_train)

print("Test set R^2: {:.2f}".format(lr.score(X_test, y_test)))


# In[5]:


from sklearn.preprocessing import PolynomialFeatures

X_train = X_train.reshape(-1,1)
X_test = X_test.reshape(-1,1)
X_valid = X_valid.reshape(-1,1)

for degree in [2, 3, 4, 5]:
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    poly.fit(X_train) 
    X_poly_train = poly.transform(X_train)
    X_poly_test = poly.transform(X_test)
    X_poly_valid = poly.transform(X_valid)
    poly_reg = LinearRegression().fit(X_poly_train, y_train)
    poly_score = poly_reg.score(X_poly_valid, y_valid)
    print("Evaluation on a valid set: {:.2f}".format(poly_score))

# degree of 2 has the best results, so we choose it
poly = PolynomialFeatures(degree=2, include_bias=False)
poly.fit(X_train) 
X_poly_train = poly.transform(X_train)
X_poly_test = poly.transform(X_test)
X_poly_valid = poly.transform(X_valid)
poly_reg = LinearRegression().fit(X_poly_train, y_train)
lrscore = poly_reg.score(X_poly_valid, y_valid)    
    
poly_reg2 = LinearRegression().fit(X_poly_valid, y_valid)
poly_score2 = poly_reg2.score(X_poly_test, y_test)
print("Evaluation on a test set: {:.2f}".format(poly_score2))    


# In[6]:


from sklearn.neighbors import KNeighborsRegressor
for x in [1, 3, 6, 10]:
    knn_reg = KNeighborsRegressor(n_neighbors=x)
    knn_reg.fit(X_train, y_train)
    print("Evaluation on a valid set: {:.2f}".format(knn_reg.score(X_valid, y_valid)))

# neighbors of 6 has the best results, so we choose it
knn_reg2 = KNeighborsRegressor(n_neighbors=6)
knn_reg2 = knn_reg2.fit(X_valid, y_valid)
knn_score2 = knn_reg2.score(X_test, y_test)
print("Evaluation on a test set: {:.2f}".format(knn_score2))


# In[7]:


#plt.scatter(X_train, X_test, color="black")
line = np.linspace(0, 3, 1000).reshape(-1, 1)

plt.figure()
plt.plot(X_train,y_train,'b.')
plt.plot(X_test,y_test,'r.')
yPred = lr.coef_*X_train +lr.intercept_
plt.plot(X_train,yPred,'g' )
plt.plot(X_train,poly_reg2.predict( poly.transform(X_train)),'+k')
plt.plot(line, knn_reg2.predict(line), 'y')
plt.xlabel('height (m)')
plt.ylabel('weight (kg)')
plt.title("Regression comparison")
print("Best regression model: KNN regression")
print("Evaluation on a test set: {:.2f}".format(poly_score2))    


# In[8]:


import pandas as pd

df = pd.read_csv("2108387_pokemon.csv") 
X = df.drop(columns=["hp", "name"])
target = df[["hp"]] #y variable
y = target


# In[9]:


import seaborn as sns
import matplotlib.pyplot as plt

df.corr()
plt.figure(figsize=(16, 6))
heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);

#correlation can be negative, in order to find the highest value through sorting we need to eliminate "-"
hpcorr = abs(df.corr()[['hp']])
print(hpcorr.sort_values(by='hp', ascending=False))
print("Feature with the highest correlation rate:", "capture_rate")


# In[10]:


df['type'].value_counts().plot(kind='bar')
plt.title("number of pokemons of each type")


# In[11]:


X = pd.get_dummies(X)
display(X)


# In[12]:


from sklearn.model_selection import train_test_split

X_train, X_rem, y_train, y_rem = train_test_split(X,y, random_state=42, train_size=0.8)

# Now since we want the valid and test size to be equal (10% each of overall data). 
# we have to define valid_size=0.5 (that is 50% of remaining data)
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, random_state=42, test_size=0.5)

print(X_train.shape, y_train.shape)
print(X_valid.shape, y_valid.shape)
print(X_test.shape, y_test.shape)


# In[13]:


from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LinearRegression

alpha_value = []
r_square = []
data = []

lr = LinearRegression().fit(X_train, y_train)

dummy_clf = DummyClassifier(strategy="uniform")
dummy = dummy_clf.fit(X_train, y_train)

score = lr.score(X_test, y_test)
score = round(score, 2)
alpha_value.append(0)
r_square.append(score)
data.append("Linear regression")
print("Test set R^2 for linear regression", score)
print("Test set R^2 for dummy regressor: {:.2f}".format(dummy.score(X_test, y_test)))


# In[14]:


from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso

for alpha in [0.001, 0.01, 0.1, 1, 10, 100]:
    ridge = Ridge(alpha=alpha).fit(X_train, y_train)
    score = ridge.score(X_valid, y_valid)
    score = round(score, 2)
    alpha_value.append(alpha)
    r_square.append(score)
    data.append("Ridge regression")
    print("Ridge test set score", score, "where alpha:",alpha)
    lasso = Lasso(alpha=10, max_iter=100000).fit(X_train, y_train)
    score = lasso.score(X_valid, y_valid)
    score = round(score, 2)
    alpha_value.append(alpha)
    r_square.append(score)
    data.append("Lasso regression")
    print("Lasso test set score", score, "where alpha:",alpha)

#results are very similar for different alphas, so 0.001 is taken for ridge and lasso evaluation
ridge = Ridge(alpha=0.001).fit(X_valid, y_valid)
score = ridge.score(X_test, y_test)
print("Ridge evaluation on a test set:", score) 

lasso = Lasso(alpha=0.001, max_iter=100000).fit(X_valid, y_valid)
score = lasso.score(X_test, y_test)
print("Lasso evaluation on a test set:", score)


# In[15]:


data2 = list(zip(data, alpha_value, r_square))

df_new = pd.DataFrame(data2, columns=["Type of the regression model", "Alphavalue", "R-squared score"])

print(df_new)

df_new.to_csv('regression_dataframe.csv') 

